{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d26236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cross-Species Fruit Quality Grading via Few-Shot Prototypical Networks\n",
    "# Learning Class-Agnostic Defect Representations\n",
    "# \n",
    "# Author: Amr Samir\n",
    "# Master's Thesis - 2026\n",
    "# ============================================================================\n",
    "# \n",
    "# RESEARCH GAP: \n",
    "# While models can classify fruit species, they fail to generalize quality \n",
    "# grading (Good/Bad) across unseen fruit types. This work proves that metric \n",
    "# learning can learn \"defectness\" rather than \"fruit-specific features.\"\n",
    "#\n",
    "# KEY CONTRIBUTION:\n",
    "# Train on {Apple, Banana, Grape} ‚Üí Test on {Mango, Orange} WITHOUT retraining\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa87b68",
   "metadata": {},
   "source": [
    "## 1. Configuration & Hyperparameters\n",
    "\n",
    "Define all experimental settings in one place for reproducibility and easy ablation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c203b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENTAL CONFIGURATION\n",
      "============================================================\n",
      "Train Fruits (SEEN):     ['apple', 'banana', 'grape']\n",
      "Test Fruits (UNSEEN):    ['mango', 'orange']\n",
      "Few-Shot Setting:        5-shot, 15-query\n",
      "Backbone:                resnet18\n",
      "Embedding Dimension:     512\n",
      "Training Episodes:       1000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these based on your dataset and experiments\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration for all experiments\"\"\"\n",
    "    \n",
    "    # Dataset paths - FruitVision Dataset\n",
    "    DATA_ROOT = r\"D:\\Datasets\\FruitVision\"\n",
    "    \n",
    "    # Fruits for training (SEEN during training)\n",
    "    TRAIN_FRUITS = ['apple', 'banana', 'grape']\n",
    "    \n",
    "    # Fruits for testing (UNSEEN - the key experiment!)\n",
    "    TEST_FRUITS = ['mango', 'orange']\n",
    "    \n",
    "    # Quality classes (binary grading) - FruitVision uses fresh/rotten\n",
    "    CLASSES = ['fresh', 'rotten']  # Maps to Good/Bad\n",
    "    N_CLASSES = 2  # Binary: Fresh (Good) vs Rotten (Bad)\n",
    "    \n",
    "    # Few-shot settings\n",
    "    N_SHOT = 5       # Number of support examples per class (5-shot learning)\n",
    "    N_QUERY = 15     # Number of query examples per class\n",
    "    N_EPISODES_TRAIN = 1000   # Training episodes per epoch\n",
    "    N_EPISODES_VAL = 200      # Validation episodes\n",
    "    N_EPISODES_TEST = 600     # Test episodes for statistical significance\n",
    "    \n",
    "    # Model settings\n",
    "    BACKBONE = 'resnet18'     # Options: 'resnet18', 'resnet50', 'vit_tiny'\n",
    "    EMBEDDING_DIM = 512       # Dimension of the embedding space\n",
    "    PRETRAINED = True         # Use ImageNet pretrained weights\n",
    "    \n",
    "    # Training settings\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    BATCH_SIZE = 1            # For episodic training, batch_size = 1 episode\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 224\n",
    "    \n",
    "    # Paths for saving\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    RESULTS_DIR = './results'\n",
    "    \n",
    "    # Experiment name (for logging)\n",
    "    EXPERIMENT_NAME = f\"ProtoNet_{BACKBONE}_{N_SHOT}shot\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTAL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train Fruits (SEEN):     {config.TRAIN_FRUITS}\")\n",
    "print(f\"Test Fruits (UNSEEN):    {config.TEST_FRUITS}\")\n",
    "print(f\"Few-Shot Setting:        {config.N_SHOT}-shot, {config.N_QUERY}-query\")\n",
    "print(f\"Backbone:                {config.BACKBONE}\")\n",
    "print(f\"Embedding Dimension:     {config.EMBEDDING_DIM}\")\n",
    "print(f\"Training Episodes:       {config.N_EPISODES_TRAIN}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60821e2a",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation & Transforms\n",
    "\n",
    "Strong augmentation is critical for learning generalizable defect features. We use different transforms for support (stable) and query (augmented) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaff842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data transforms defined\n",
      "  Training: Strong augmentation with color jitter, rotation, erasing\n",
      "  Evaluation: Minimal transforms (resize + normalize)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA AUGMENTATION STRATEGIES\n",
    "# ============================================================================\n",
    "# Key insight: Defects have texture/edge patterns. Augmentations should preserve\n",
    "# these while varying lighting, orientation, and scale.\n",
    "\n",
    "# Training augmentation (strong)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.3,\n",
    "        contrast=0.3,\n",
    "        saturation=0.3,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1))  # Simulates occlusion\n",
    "])\n",
    "\n",
    "# Validation/Test augmentation (minimal - just normalization)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"‚úì Data transforms defined\")\n",
    "print(f\"  Training: Strong augmentation with color jitter, rotation, erasing\")\n",
    "print(f\"  Evaluation: Minimal transforms (resize + normalize)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c3c2d",
   "metadata": {},
   "source": [
    "## 3. Episodic Dataset for Few-Shot Learning\n",
    "\n",
    "The core innovation: Instead of traditional batches, we sample **episodes**. Each episode contains:\n",
    "- **Support Set**: K examples of Good + K examples of Bad (used to build prototypes)\n",
    "- **Query Set**: Q examples to classify using the prototypes\n",
    "\n",
    "This forces the model to learn \"defectness\" in a generalizable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e17ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Episodic dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EPISODIC DATASET FOR FEW-SHOT QUALITY GRADING\n",
    "# ============================================================================\n",
    "\n",
    "class FruitQualityDataset:\n",
    "    \"\"\"\n",
    "    Loads fruit images organized by fruit type and quality.\n",
    "    Expected structure (FruitVision):\n",
    "        data_root/\n",
    "            fruit_name/\n",
    "                fresh/\n",
    "                    img1.jpg, img2.jpg, ...\n",
    "                rotten/\n",
    "                    img1.jpg, img2.jpg, ...\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, fruit_types, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.fruit_types = fruit_types\n",
    "        self.transform = transform\n",
    "        self.classes = ['fresh', 'rotten']  # FruitVision naming\n",
    "        \n",
    "        # Organize images by fruit and quality\n",
    "        self.data = defaultdict(lambda: defaultdict(list))\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all image paths organized by fruit type and quality\"\"\"\n",
    "        for fruit in self.fruit_types:\n",
    "            for quality in self.classes:\n",
    "                folder_path = os.path.join(self.data_root, fruit, quality)\n",
    "                if os.path.exists(folder_path):\n",
    "                    images = [\n",
    "                        os.path.join(folder_path, f) \n",
    "                        for f in os.listdir(folder_path) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "                    ]\n",
    "                    self.data[fruit][quality] = images\n",
    "                    print(f\"  Loaded {len(images):4d} images: {fruit}/{quality}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö† Missing folder: {folder_path}\")\n",
    "    \n",
    "    def get_episode(self, n_shot, n_query, fruit=None):\n",
    "        \"\"\"\n",
    "        Sample a single episode for few-shot learning.\n",
    "        \n",
    "        Args:\n",
    "            n_shot: Number of support examples per class\n",
    "            n_query: Number of query examples per class\n",
    "            fruit: Specific fruit to sample from (None = random)\n",
    "            \n",
    "        Returns:\n",
    "            support_images: [n_classes * n_shot] tensor\n",
    "            support_labels: [n_classes * n_shot] tensor\n",
    "            query_images: [n_classes * n_query] tensor\n",
    "            query_labels: [n_classes * n_query] tensor\n",
    "            fruit_name: Name of the fruit in this episode\n",
    "        \"\"\"\n",
    "        # Select fruit\n",
    "        if fruit is None:\n",
    "            fruit = random.choice(self.fruit_types)\n",
    "        \n",
    "        support_images, support_labels = [], []\n",
    "        query_images, query_labels = [], []\n",
    "        \n",
    "        for class_idx, quality in enumerate(self.classes):\n",
    "            # Get all images for this fruit-quality combination\n",
    "            all_images = self.data[fruit][quality]\n",
    "            \n",
    "            if len(all_images) < n_shot + n_query:\n",
    "                raise ValueError(\n",
    "                    f\"Not enough images for {fruit}/{quality}. \"\n",
    "                    f\"Need {n_shot + n_query}, have {len(all_images)}\"\n",
    "                )\n",
    "            \n",
    "            # Randomly sample support and query\n",
    "            sampled = random.sample(all_images, n_shot + n_query)\n",
    "            support_paths = sampled[:n_shot]\n",
    "            query_paths = sampled[n_shot:]\n",
    "            \n",
    "            # Load and transform images\n",
    "            for path in support_paths:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_images.append(img)\n",
    "                support_labels.append(class_idx)\n",
    "                \n",
    "            for path in query_paths:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_images.append(img)\n",
    "                query_labels.append(class_idx)\n",
    "        \n",
    "        # Stack into tensors\n",
    "        support_images = torch.stack(support_images)  # [n_classes*n_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_images = torch.stack(query_images)      # [n_classes*n_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, support_labels, query_images, query_labels, fruit\n",
    "\n",
    "\n",
    "class EpisodicDataLoader:\n",
    "    \"\"\"\n",
    "    DataLoader that yields episodes instead of batches.\n",
    "    Each iteration returns one episode (support + query sets).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, n_shot, n_query, n_episodes, fruits=None):\n",
    "        self.dataset = dataset\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        self.fruits = fruits if fruits else dataset.fruit_types\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_episodes):\n",
    "            fruit = random.choice(self.fruits)\n",
    "            yield self.dataset.get_episode(\n",
    "                self.n_shot, \n",
    "                self.n_query, \n",
    "                fruit=fruit\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "print(\"‚úì Episodic dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bbbf9",
   "metadata": {},
   "source": [
    "## 4. Prototypical Network Architecture\n",
    "\n",
    "The **Prototypical Network** computes a prototype (centroid) for each class from support examples, then classifies queries by distance to prototypes.\n",
    "\n",
    "$$d(z_q, c_k) = \\|f_\\theta(x_q) - \\frac{1}{|S_k|}\\sum_{x_i \\in S_k} f_\\theta(x_i)\\|^2$$\n",
    "\n",
    "Where:\n",
    "- $f_\\theta$ is our embedding network (ResNet backbone)\n",
    "- $c_k$ is the prototype for class $k$\n",
    "- $z_q$ is the query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560261dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model initialized: resnet18\n",
      "  Total parameters:     11,702,848\n",
      "  Trainable parameters: 11,702,848\n",
      "  Embedding dimension:  512\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROTOTYPICAL NETWORK ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extraction backbone that maps images to embedding space.\n",
    "    The embedding should capture defect-related features (texture, edges, anomalies).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='resnet18', embedding_dim=512, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            in_features = self.encoder.fc.in_features\n",
    "            self.encoder.fc = nn.Identity()  # Remove classification head\n",
    "            \n",
    "        elif backbone == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            in_features = self.encoder.fc.in_features\n",
    "            self.encoder.fc = nn.Identity()\n",
    "            \n",
    "        elif backbone == 'efficientnet_b0':\n",
    "            self.encoder = models.efficientnet_b0(pretrained=pretrained)\n",
    "            in_features = self.encoder.classifier[1].in_features\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        # Projection head to embedding dimension\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract embeddings from images\"\"\"\n",
    "        features = self.encoder(x)\n",
    "        embeddings = self.projection(features)\n",
    "        # L2 normalize embeddings (important for metric learning)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Few-Shot Prototypical Network for Fruit Quality Grading.\n",
    "    \n",
    "    Key idea: Build a prototype (mean embedding) for each class from support \n",
    "    examples, then classify queries by distance to prototypes.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='resnet18', embedding_dim=512, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder = EmbeddingNetwork(backbone, embedding_dim, pretrained)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def compute_prototypes(self, support_embeddings, support_labels, n_classes=2):\n",
    "        \"\"\"\n",
    "        Compute class prototypes from support set.\n",
    "        \n",
    "        Args:\n",
    "            support_embeddings: [n_support, embedding_dim]\n",
    "            support_labels: [n_support]\n",
    "            n_classes: Number of classes\n",
    "            \n",
    "        Returns:\n",
    "            prototypes: [n_classes, embedding_dim]\n",
    "        \"\"\"\n",
    "        prototypes = torch.zeros(n_classes, self.embedding_dim, device=support_embeddings.device)\n",
    "        \n",
    "        for c in range(n_classes):\n",
    "            mask = (support_labels == c)\n",
    "            class_embeddings = support_embeddings[mask]\n",
    "            prototypes[c] = class_embeddings.mean(dim=0)\n",
    "        \n",
    "        return prototypes\n",
    "    \n",
    "    def forward(self, support_images, support_labels, query_images, n_classes=2):\n",
    "        \"\"\"\n",
    "        Forward pass for one episode.\n",
    "        \n",
    "        Args:\n",
    "            support_images: [n_support, C, H, W]\n",
    "            support_labels: [n_support]\n",
    "            query_images: [n_query, C, H, W]\n",
    "            n_classes: Number of classes\n",
    "            \n",
    "        Returns:\n",
    "            logits: [n_query, n_classes] - negative distances (for softmax)\n",
    "            query_embeddings: [n_query, embedding_dim]\n",
    "            prototypes: [n_classes, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        support_embeddings = self.encoder(support_images)  # [n_support, dim]\n",
    "        query_embeddings = self.encoder(query_images)      # [n_query, dim]\n",
    "        \n",
    "        # Compute prototypes\n",
    "        prototypes = self.compute_prototypes(\n",
    "            support_embeddings, support_labels, n_classes\n",
    "        )  # [n_classes, dim]\n",
    "        \n",
    "        # Compute distances from queries to prototypes\n",
    "        # Using squared Euclidean distance\n",
    "        distances = torch.cdist(query_embeddings, prototypes, p=2)  # [n_query, n_classes]\n",
    "        \n",
    "        # Return negative distances as logits (closer = higher probability)\n",
    "        logits = -distances\n",
    "        \n",
    "        return logits, query_embeddings, prototypes\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = PrototypicalNetwork(\n",
    "    backbone=config.BACKBONE,\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    pretrained=config.PRETRAINED\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model initialized: {config.BACKBONE}\")\n",
    "print(f\"  Total parameters:     {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Embedding dimension:  {config.EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e168812",
   "metadata": {},
   "source": [
    "## 5. Supervised Contrastive Loss (Alternative)\n",
    "\n",
    "We implement **SupCon Loss** as an alternative to standard cross-entropy. This loss pushes embeddings of the same class together and different classes apart, which is crucial for learning generalizable defect features.\n",
    "\n",
    "$$\\mathcal{L}_{SupCon} = \\sum_{i} \\frac{-1}{|P(i)|} \\sum_{p \\in P(i)} \\log \\frac{\\exp(z_i \\cdot z_p / \\tau)}{\\sum_{a \\neq i} \\exp(z_i \\cdot z_a / \\tau)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca4e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loss functions defined\n",
      "  Using: Combined Prototypical + Supervised Contrastive Loss\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised Contrastive Loss (Khosla et al., 2020)\n",
    "    \n",
    "    This loss is particularly good for learning embeddings that generalize\n",
    "    across different visual domains (different fruit types).\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, embedding_dim] - L2 normalized embeddings\n",
    "            labels: [batch_size]\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        # Compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T),\n",
    "            self.temperature\n",
    "        )\n",
    "        \n",
    "        # For numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        \n",
    "        # Mask out self-contrast\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-6)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "class PrototypicalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Prototypical Network loss (cross-entropy on distances).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [n_query, n_classes] - negative distances\n",
    "            labels: [n_query]\n",
    "        \"\"\"\n",
    "        return self.ce(logits, labels)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines prototypical loss with supervised contrastive loss.\n",
    "    This often improves embedding quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, proto_weight=1.0, supcon_weight=0.5, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.proto_loss = PrototypicalLoss()\n",
    "        self.supcon_loss = SupConLoss(temperature)\n",
    "        self.proto_weight = proto_weight\n",
    "        self.supcon_weight = supcon_weight\n",
    "        \n",
    "    def forward(self, logits, query_labels, all_embeddings, all_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [n_query, n_classes]\n",
    "            query_labels: [n_query]\n",
    "            all_embeddings: [n_support + n_query, dim]\n",
    "            all_labels: [n_support + n_query]\n",
    "        \"\"\"\n",
    "        loss_proto = self.proto_loss(logits, query_labels)\n",
    "        loss_supcon = self.supcon_loss(all_embeddings, all_labels)\n",
    "        \n",
    "        total_loss = (self.proto_weight * loss_proto + \n",
    "                      self.supcon_weight * loss_supcon)\n",
    "        \n",
    "        return total_loss, loss_proto, loss_supcon\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = CombinedLoss(\n",
    "    proto_weight=1.0, \n",
    "    supcon_weight=0.5, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"‚úì Loss functions defined\")\n",
    "print(\"  Using: Combined Prototypical + Supervised Contrastive Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f0309",
   "metadata": {},
   "source": [
    "## 6. Training Loop (Episodic Training)\n",
    "\n",
    "Unlike traditional training, we iterate over **episodes** not batches. Each episode simulates a few-shot scenario, forcing the model to learn generalizable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b070a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training utilities defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    \"\"\"Compute accuracy from logits\"\"\"\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    accuracy = (predictions == labels).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch (many episodes)\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_episodes = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for support_imgs, support_lbls, query_imgs, query_lbls, fruit in pbar:\n",
    "        # Move to device\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_lbls = support_lbls.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        query_lbls = query_lbls.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, query_emb, prototypes = model(\n",
    "            support_imgs, support_lbls, query_imgs\n",
    "        )\n",
    "        \n",
    "        # Get support embeddings for SupCon loss\n",
    "        support_emb = model.encoder(support_imgs)\n",
    "        all_embeddings = torch.cat([support_emb, query_emb], dim=0)\n",
    "        all_labels = torch.cat([support_lbls, query_lbls], dim=0)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_proto, loss_supcon = criterion(\n",
    "            logits, query_lbls, all_embeddings, all_labels\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = compute_accuracy(logits, query_lbls)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        n_episodes += 1\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{acc:.3f}',\n",
    "            'fruit': fruit\n",
    "        })\n",
    "    \n",
    "    return total_loss / n_episodes, total_acc / n_episodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, desc=\"Evaluating\"):\n",
    "    \"\"\"Evaluate on validation/test episodes\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    n_episodes = 0\n",
    "    fruit_accuracies = defaultdict(list)\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    for support_imgs, support_lbls, query_imgs, query_lbls, fruit in pbar:\n",
    "        # Move to device\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_lbls = support_lbls.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        query_lbls = query_lbls.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _, _ = model(support_imgs, support_lbls, query_imgs)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        acc = compute_accuracy(logits, query_lbls)\n",
    "        total_acc += acc\n",
    "        n_episodes += 1\n",
    "        fruit_accuracies[fruit].append(acc)\n",
    "        \n",
    "        pbar.set_postfix({'acc': f'{acc:.3f}', 'fruit': fruit})\n",
    "    \n",
    "    # Compute per-fruit accuracy\n",
    "    per_fruit_acc = {\n",
    "        fruit: np.mean(accs) for fruit, accs in fruit_accuracies.items()\n",
    "    }\n",
    "    \n",
    "    return total_acc / n_episodes, per_fruit_acc\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }, path)\n",
    "    print(f\"  ‚úì Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['metrics']\n",
    "\n",
    "print(\"‚úì Training utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a95f86",
   "metadata": {},
   "source": [
    "## 7. Load Dataset & Create DataLoaders\n",
    "\n",
    "**Dataset:** FruitVision (D:\\Datasets\\FruitVision)\n",
    "\n",
    "Dataset structure:\n",
    "```\n",
    "FruitVision/\n",
    "‚îú‚îÄ‚îÄ apple/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ fresh/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rotten/\n",
    "‚îú‚îÄ‚îÄ banana/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ fresh/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rotten/\n",
    "‚îú‚îÄ‚îÄ grape/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ fresh/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rotten/\n",
    "‚îú‚îÄ‚îÄ mango/          ‚Üê UNSEEN (test only)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ fresh/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rotten/\n",
    "‚îî‚îÄ‚îÄ orange/         ‚Üê UNSEEN (test only)\n",
    "    ‚îú‚îÄ‚îÄ fresh/\n",
    "    ‚îî‚îÄ‚îÄ rotten/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab8cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATASETS\n",
      "============================================================\n",
      "Data root: D:\\Datasets\\FruitVision\n",
      "\n",
      "Loading TRAINING data (seen fruits):\n",
      "  Loaded  765 images: apple/fresh\n",
      "  Loaded  630 images: apple/rotten\n",
      "  Loaded  749 images: banana/fresh\n",
      "  Loaded  632 images: banana/rotten\n",
      "  Loaded  770 images: grape/fresh\n",
      "  Loaded  630 images: grape/rotten\n",
      "\n",
      "Loading TEST data (UNSEEN fruits - key experiment!):\n",
      "  Loaded  763 images: mango/fresh\n",
      "  Loaded  630 images: mango/rotten\n",
      "  Loaded  753 images: orange/fresh\n",
      "  Loaded  656 images: orange/rotten\n",
      "\n",
      "============================================================\n",
      "DataLoaders created:\n",
      "  Train: 1000 episodes on ['apple', 'banana', 'grape']\n",
      "  Val:   200 episodes on ['apple', 'banana', 'grape']\n",
      "  Test:  600 episodes on ['mango', 'orange'] (UNSEEN!)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATASET\n",
    "# ============================================================================\n",
    "# NOTE: Update config.DATA_ROOT to your actual dataset path!\n",
    "# You can download datasets from:\n",
    "# 1. FruitVision (Kaggle): https://www.kaggle.com/datasets/fruitvision\n",
    "# 2. Zenodo Fruit Quality: https://zenodo.org/records/1310165\n",
    "# 3. Fruits Fresh and Rotten: https://www.kaggle.com/datasets/sriramr/fruits-fresh-and-rotten-for-classification\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Data root: {config.DATA_ROOT}\")\n",
    "print()\n",
    "\n",
    "# Check if data exists\n",
    "if not os.path.exists(config.DATA_ROOT):\n",
    "    print(\"‚ö†Ô∏è  WARNING: Dataset not found!\")\n",
    "    print(f\"   Please ensure FruitVision is at: {config.DATA_ROOT}\")\n",
    "    print()\n",
    "    print(\"   Expected structure:\")\n",
    "    print(\"   FruitVision/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ apple/\")\n",
    "    print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ fresh/\")\n",
    "    print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ rotten/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ banana/\")\n",
    "    print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ fresh/\")\n",
    "    print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ rotten/\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ ...\")\n",
    "else:\n",
    "    # Load training dataset (SEEN fruits)\n",
    "    print(\"Loading TRAINING data (seen fruits):\")\n",
    "    train_dataset = FruitQualityDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        fruit_types=config.TRAIN_FRUITS,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    print(\"Loading TEST data (UNSEEN fruits - key experiment!):\")\n",
    "    test_dataset = FruitQualityDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        fruit_types=config.TEST_FRUITS,\n",
    "        transform=eval_transform\n",
    "    )\n",
    "    \n",
    "    # Create episodic dataloaders\n",
    "    train_loader = EpisodicDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        n_shot=config.N_SHOT,\n",
    "        n_query=config.N_QUERY,\n",
    "        n_episodes=config.N_EPISODES_TRAIN\n",
    "    )\n",
    "    \n",
    "    val_loader = EpisodicDataLoader(\n",
    "        dataset=train_dataset,  # Validation on seen fruits\n",
    "        n_shot=config.N_SHOT,\n",
    "        n_query=config.N_QUERY,\n",
    "        n_episodes=config.N_EPISODES_VAL\n",
    "    )\n",
    "    \n",
    "    # KEY EXPERIMENT: Test on UNSEEN fruits\n",
    "    test_loader = EpisodicDataLoader(\n",
    "        dataset=test_dataset,  # Unseen fruits!\n",
    "        n_shot=config.N_SHOT,\n",
    "        n_query=config.N_QUERY,\n",
    "        n_episodes=config.N_EPISODES_TEST\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DataLoaders created:\")\n",
    "    print(f\"  Train: {len(train_loader)} episodes on {config.TRAIN_FRUITS}\")\n",
    "    print(f\"  Val:   {len(val_loader)} episodes on {config.TRAIN_FRUITS}\")\n",
    "    print(f\"  Test:  {len(test_loader)} episodes on {config.TEST_FRUITS} (UNSEEN!)\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a2a79",
   "metadata": {},
   "source": [
    "## 8. Main Training Loop\n",
    "\n",
    "Run the full training with validation. The model learns from episodes of Apple, Banana, Grape (seen fruits) and will be tested on Mango, Orange (unseen fruits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c2a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training on SEEN fruits: ['apple', 'banana', 'grape']\n",
      "Will test on UNSEEN fruits: ['mango', 'orange']\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/1000 [12:24:20<50:33,  6.96s/it, loss=2.1887, acc=0.900, fruit=apple]        "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def train_protonet(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"\n",
    "    Full training loop with validation and checkpointing.\n",
    "    \"\"\"\n",
    "    # Optimizer with different learning rates\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.encoder.encoder.parameters(), 'lr': config.LEARNING_RATE * 0.1},  # Backbone\n",
    "        {'params': model.encoder.projection.parameters(), 'lr': config.LEARNING_RATE}       # Projection\n",
    "    ], weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.EPOCHS, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = CombinedLoss(proto_weight=1.0, supcon_weight=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_acc': [], 'val_per_fruit': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training on SEEN fruits: {config.TRAIN_FRUITS}\")\n",
    "    print(f\"Will test on UNSEEN fruits: {config.TEST_FRUITS}\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_acc, val_per_fruit = evaluate(\n",
    "            model, val_loader, device, desc=\"Validating\"\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_per_fruit'].append(val_per_fruit)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "        print(f\"  Val Acc:    {val_acc:.3f}\")\n",
    "        print(f\"  Per-fruit:  {val_per_fruit}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint(\n",
    "                model, optimizer, epoch,\n",
    "                {'val_acc': val_acc, 'per_fruit': val_per_fruit},\n",
    "                os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "            )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TRAINING COMPLETE\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Uncomment to train (only run if dataset exists)\n",
    "history = train_protonet(model, train_loader, val_loader, config, device)\n",
    "\n",
    "print(\"‚úì Training function defined\")\n",
    "print(\"  To start training, run:\")\n",
    "print(\"  history = train_protonet(model, train_loader, val_loader, config, device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f6a8d",
   "metadata": {},
   "source": [
    "## 9. KEY EXPERIMENT: Test on UNSEEN Fruits üéØ\n",
    "\n",
    "**This is the core contribution of your thesis!**\n",
    "\n",
    "The model was trained ONLY on Apple, Banana, Grape. Now we test on Mango and Orange (which the model has NEVER seen). If it performs well, we have proven that it learned \"defectness\" rather than fruit-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Key experiment function defined\n",
      "  To run: results = test_on_unseen_fruits(model, test_loader, device, config)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KEY EXPERIMENT: TEST ON UNSEEN FRUITS\n",
    "# ============================================================================\n",
    "\n",
    "def test_on_unseen_fruits(model, test_loader, device, config):\n",
    "    \"\"\"\n",
    "    The critical experiment that validates our research gap.\n",
    "    \n",
    "    Model trained on: Apple, Banana, Grape\n",
    "    Testing on: Mango, Orange (NEVER seen during training!)\n",
    "    \n",
    "    If accuracy > 80%, we have proven cross-species generalization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ KEY EXPERIMENT: TESTING ON UNSEEN FRUIT SPECIES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model was trained on: {config.TRAIN_FRUITS}\")\n",
    "    print(f\"Testing on (UNSEEN): {config.TEST_FRUITS}\")\n",
    "    print(f\"Few-shot setting: {config.N_SHOT}-shot\")\n",
    "    print()\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"‚úì Loaded best model from training\")\n",
    "    else:\n",
    "        print(\"‚ö† No checkpoint found, using current model weights\")\n",
    "    \n",
    "    # Evaluate on unseen fruits\n",
    "    test_acc, per_fruit_acc = evaluate(\n",
    "        model, test_loader, device, \n",
    "        desc=\"Testing on UNSEEN fruits\"\n",
    "    )\n",
    "    \n",
    "    # Compute 95% confidence interval\n",
    "    # Run multiple times for statistical significance\n",
    "    print(\"\\nComputing confidence intervals (600 episodes)...\")\n",
    "    all_accuracies = []\n",
    "    per_fruit_all = defaultdict(list)\n",
    "    \n",
    "    for _ in tqdm(range(5), desc=\"Trials\"):\n",
    "        acc, per_fruit = evaluate(model, test_loader, device, desc=\"Trial\")\n",
    "        all_accuracies.append(acc)\n",
    "        for fruit, fruit_acc in per_fruit.items():\n",
    "            per_fruit_all[fruit].append(fruit_acc)\n",
    "    \n",
    "    mean_acc = np.mean(all_accuracies)\n",
    "    std_acc = np.std(all_accuracies)\n",
    "    ci_95 = 1.96 * std_acc / np.sqrt(len(all_accuracies))\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä RESULTS ON UNSEEN FRUITS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nOverall Accuracy: {mean_acc:.3f} ¬± {ci_95:.3f} (95% CI)\")\n",
    "    print(f\"\\nPer-Fruit Accuracy:\")\n",
    "    \n",
    "    for fruit in config.TEST_FRUITS:\n",
    "        fruit_mean = np.mean(per_fruit_all[fruit])\n",
    "        fruit_std = np.std(per_fruit_all[fruit])\n",
    "        print(f\"  {fruit.capitalize()}: {fruit_mean:.3f} ¬± {fruit_std:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    if mean_acc > 0.80:\n",
    "        print(\"‚úÖ SUCCESS: Model generalizes to unseen fruits!\")\n",
    "        print(\"   This validates our hypothesis that metric learning can learn\")\n",
    "        print(\"   'class-agnostic defect representations'.\")\n",
    "    elif mean_acc > 0.65:\n",
    "        print(\"‚ö†Ô∏è  PARTIAL SUCCESS: Better than random (50%), room for improvement\")\n",
    "    else:\n",
    "        print(\"‚ùå Model struggles to generalize. Consider:\")\n",
    "        print(\"   - More diverse training fruits\")\n",
    "        print(\"   - Stronger augmentation\")\n",
    "        print(\"   - Different backbone\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'std': std_acc,\n",
    "        'ci_95': ci_95,\n",
    "        'per_fruit': dict(per_fruit_all)\n",
    "    }\n",
    "\n",
    "# Uncomment to run experiment (only after training)\n",
    "results = test_on_unseen_fruits(model, test_loader, device, config)\n",
    "\n",
    "print(\"‚úì Key experiment function defined\")\n",
    "print(\"  To run: results = test_on_unseen_fruits(model, test_loader, device, config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7fdde",
   "metadata": {},
   "source": [
    "## 10. Baseline Comparisons (For Paper)\n",
    "\n",
    "To validate your contribution, you must compare against:\n",
    "1. **Supervised CNN** (ResNet trained on all fruits)\n",
    "2. **Transfer Learning** (Pretrained ResNet, fine-tuned)\n",
    "3. **Zero-Shot CLIP** (No training examples)\n",
    "4. **Standard Prototypical Network** (Without SupCon loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Baseline comparison functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BASELINE COMPARISONS\n",
    "# ============================================================================\n",
    "\n",
    "# Baseline 1: Supervised CNN (Upper Bound)\n",
    "class SupervisedBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard supervised classifier trained on all fruits.\n",
    "    This is the UPPER BOUND - it sees all data during training.\n",
    "    Your few-shot model should approach this performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='resnet18', num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        if backbone == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=pretrained)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Baseline 2: Zero-Shot CLIP (if available)\n",
    "def create_clip_baseline():\n",
    "    \"\"\"\n",
    "    Zero-shot CLIP baseline using text prompts.\n",
    "    Install: pip install transformers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        \n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        return model, processor\n",
    "    except ImportError:\n",
    "        print(\"‚ö† Install transformers for CLIP baseline: pip install transformers\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def clip_zero_shot_classify(image, model, processor, device):\n",
    "    \"\"\"\n",
    "    Classify fruit quality using CLIP zero-shot.\n",
    "    \"\"\"\n",
    "    # Text prompts for quality\n",
    "    text_prompts = [\n",
    "        \"a photo of a fresh, healthy fruit\",\n",
    "        \"a photo of a rotten, defective fruit\"\n",
    "    ]\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=text_prompts, \n",
    "        images=image, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    return probs.argmax().item()\n",
    "\n",
    "\n",
    "# Baseline 3: Standard ProtoNet (without SupCon)\n",
    "class StandardProtoNet(PrototypicalNetwork):\n",
    "    \"\"\"\n",
    "    Standard Prototypical Network without Supervised Contrastive Loss.\n",
    "    Used to show the benefit of adding SupCon.\n",
    "    \"\"\"\n",
    "    pass  # Same architecture, different loss function\n",
    "\n",
    "\n",
    "def run_baseline_comparisons(config, device):\n",
    "    \"\"\"\n",
    "    Run all baseline comparisons for the paper.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BASELINE COMPARISONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # TODO: Add actual baseline runs here\n",
    "    # This is a placeholder showing expected results format\n",
    "    \n",
    "    results['our_method'] = {\n",
    "        'seen_fruits': 0.95,   # Expected ~95% on seen fruits\n",
    "        'unseen_fruits': 0.85  # Expected ~85% on unseen fruits\n",
    "    }\n",
    "    \n",
    "    results['supervised_cnn'] = {\n",
    "        'seen_fruits': 0.98,   # Upper bound\n",
    "        'unseen_fruits': 0.60  # Fails to generalize\n",
    "    }\n",
    "    \n",
    "    results['clip_zero_shot'] = {\n",
    "        'seen_fruits': 0.70,   # No training\n",
    "        'unseen_fruits': 0.70  # No training\n",
    "    }\n",
    "    \n",
    "    results['standard_protonet'] = {\n",
    "        'seen_fruits': 0.92,\n",
    "        'unseen_fruits': 0.78  # Our method should beat this\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Baseline comparison functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee899f8c",
   "metadata": {},
   "source": [
    "## 11. Visualization & Analysis\n",
    "\n",
    "Essential visualizations for your paper:\n",
    "1. t-SNE of embedding space (showing clustering of Good/Bad across fruits)\n",
    "2. Training curves\n",
    "3. Confusion matrices\n",
    "4. Per-fruit performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a989d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Visualization functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_embedding_space(model, dataset, device, n_samples=200, save_path=None):\n",
    "    \"\"\"\n",
    "    Create t-SNE visualization of the embedding space.\n",
    "    This is a KEY FIGURE for your paper showing that:\n",
    "    - Good fruits cluster together (across species)\n",
    "    - Bad fruits cluster together (across species)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    fruit_types = []\n",
    "    \n",
    "    # Sample images from each fruit-quality combination\n",
    "    for fruit in dataset.fruit_types:\n",
    "        for quality_idx, quality in enumerate(['fresh', 'rotten']):\n",
    "            images = dataset.data[fruit][quality][:n_samples//len(dataset.fruit_types)//2]\n",
    "            \n",
    "            for img_path in images:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = eval_transform(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    emb = model.encoder(img_tensor)\n",
    "                \n",
    "                embeddings.append(emb.cpu().numpy().flatten())\n",
    "                labels.append(quality)\n",
    "                fruit_types.append(fruit)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Colored by quality\n",
    "    ax1 = axes[0]\n",
    "    colors = ['green' if l == 'fresh' else 'red' for l in labels]\n",
    "    ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, alpha=0.6, s=50)\n",
    "    ax1.set_title('Embedding Space by Quality\\n(Green=Fresh, Red=Rotten)', fontsize=12)\n",
    "    ax1.set_xlabel('t-SNE 1')\n",
    "    ax1.set_ylabel('t-SNE 2')\n",
    "    \n",
    "    # Plot 2: Colored by fruit type\n",
    "    ax2 = axes[1]\n",
    "    unique_fruits = list(set(fruit_types))\n",
    "    color_map = plt.cm.get_cmap('tab10')\n",
    "    colors = [color_map(unique_fruits.index(f)) for f in fruit_types]\n",
    "    \n",
    "    for fruit in unique_fruits:\n",
    "        mask = [f == fruit for f in fruit_types]\n",
    "        ax2.scatter(\n",
    "            embeddings_2d[mask, 0], \n",
    "            embeddings_2d[mask, 1], \n",
    "            label=fruit.capitalize(),\n",
    "            alpha=0.6, s=50\n",
    "        )\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Embedding Space by Fruit Type', fontsize=12)\n",
    "    ax2.set_xlabel('t-SNE 1')\n",
    "    ax2.set_ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training curves\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', color='blue')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', color='orange')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Training & Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_comparison_bar_chart(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Bar chart comparing methods - KEY FIGURE for paper.\n",
    "    \"\"\"\n",
    "    methods = list(results.keys())\n",
    "    seen_acc = [results[m]['seen_fruits'] for m in methods]\n",
    "    unseen_acc = [results[m]['unseen_fruits'] for m in methods]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, seen_acc, width, label='Seen Fruits', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, unseen_acc, width, label='Unseen Fruits', color='coral')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Cross-Species Generalization Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.replace('_', ' ').title() for m in methods])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1 + bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f8381",
   "metadata": {},
   "source": [
    "## 12. Ablation Studies (Required for Strong Paper)\n",
    "\n",
    "Ablation studies prove that each component of your method matters:\n",
    "1. **N-shot analysis**: 1-shot, 3-shot, 5-shot, 10-shot\n",
    "2. **Loss function**: ProtoLoss only vs ProtoLoss + SupCon\n",
    "3. **Backbone**: ResNet-18 vs ResNet-50 vs EfficientNet\n",
    "4. **Number of training fruits**: Train on 1, 2, 3 fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ablation study functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ABLATION STUDIES\n",
    "# ============================================================================\n",
    "\n",
    "def ablation_n_shot(model, test_dataset, device, shots=[1, 3, 5, 10], n_episodes=200):\n",
    "    \"\"\"\n",
    "    Ablation: How does performance change with number of shots?\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ABLATION: N-SHOT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for n_shot in shots:\n",
    "        print(f\"\\nTesting {n_shot}-shot...\")\n",
    "        \n",
    "        test_loader = EpisodicDataLoader(\n",
    "            dataset=test_dataset,\n",
    "            n_shot=n_shot,\n",
    "            n_query=15,\n",
    "            n_episodes=n_episodes\n",
    "        )\n",
    "        \n",
    "        acc, per_fruit = evaluate(model, test_loader, device, f\"{n_shot}-shot\")\n",
    "        results[n_shot] = {'accuracy': acc, 'per_fruit': per_fruit}\n",
    "        \n",
    "        print(f\"  {n_shot}-shot accuracy: {acc:.3f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(shots, [results[s]['accuracy'] for s in shots], 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Shots (K)')\n",
    "    plt.ylabel('Accuracy on Unseen Fruits')\n",
    "    plt.title('N-Shot Ablation Study')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(shots)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'ablation_nshot.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def ablation_loss_function(config, device):\n",
    "    \"\"\"\n",
    "    Ablation: Compare different loss functions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    losses = {\n",
    "        'proto_only': PrototypicalLoss(),\n",
    "        'proto_supcon': CombinedLoss(proto_weight=1.0, supcon_weight=0.5),\n",
    "        'supcon_heavy': CombinedLoss(proto_weight=1.0, supcon_weight=1.0),\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ABLATION: LOSS FUNCTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # TODO: Train separate models with each loss\n",
    "    # results[loss_name] = accuracy\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def ablation_training_diversity(config, device):\n",
    "    \"\"\"\n",
    "    Ablation: How does the number of training fruits affect generalization?\n",
    "    \n",
    "    - Train on 1 fruit  ‚Üí Test on unseen\n",
    "    - Train on 2 fruits ‚Üí Test on unseen  \n",
    "    - Train on 3 fruits ‚Üí Test on unseen\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ABLATION: TRAINING DIVERSITY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fruit_combinations = [\n",
    "        ['apple'],\n",
    "        ['apple', 'banana'],\n",
    "        ['apple', 'banana', 'grape'],\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for fruits in fruit_combinations:\n",
    "        print(f\"\\nTraining on: {fruits}\")\n",
    "        # TODO: Train model on subset and evaluate\n",
    "        # results[len(fruits)] = accuracy\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Ablation study functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec8760",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "### What You've Built:\n",
    "‚úÖ Prototypical Network with SupCon Loss for few-shot quality grading  \n",
    "‚úÖ Episodic training framework  \n",
    "‚úÖ Cross-species generalization experiment  \n",
    "‚úÖ Baseline comparisons structure  \n",
    "‚úÖ Visualization tools  \n",
    "\n",
    "### Dataset Download Links:\n",
    "1. **Fruits Fresh and Rotten** (Kaggle):  \n",
    "   https://www.kaggle.com/datasets/sriramr/fruits-fresh-and-rotten-for-classification\n",
    "\n",
    "2. **Fruit Quality Good/Bad** (Zenodo):  \n",
    "   https://zenodo.org/records/1310165\n",
    "\n",
    "3. **FruitVision 2025** (Search on Kaggle)\n",
    "\n",
    "### To Run:\n",
    "1. Download dataset ‚Üí Organize as shown above\n",
    "2. Update `config.DATA_ROOT` \n",
    "3. Run cells sequentially\n",
    "4. Run `train_protonet()` \n",
    "5. Run `test_on_unseen_fruits()` ‚Üê **KEY EXPERIMENT**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
