{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d26236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cross-Species Fruit Quality Grading via Few-Shot Prototypical Networks\n",
    "# Learning Class-Agnostic Defect Representations\n",
    "# \n",
    "# Author: Amr Samir\n",
    "# Master's Thesis - 2026\n",
    "# ============================================================================\n",
    "# \n",
    "# RESEARCH GAP: \n",
    "# While models can classify fruit species, they fail to generalize quality \n",
    "# grading (Good/Bad) across unseen fruit types. This work proves that metric \n",
    "# learning can learn \"defectness\" rather than \"fruit-specific features.\"\n",
    "#\n",
    "# KEY CONTRIBUTION:\n",
    "# Train on {Apple, Banana, Grape} ‚Üí Test on {Mango, Orange} WITHOUT retraining\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa87b68",
   "metadata": {},
   "source": [
    "## 1. Configuration & Hyperparameters\n",
    "\n",
    "Define all experimental settings in one place for reproducibility and easy ablation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c203b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENTAL CONFIGURATION\n",
      "============================================================\n",
      "Train Fruits (SEEN):     ['apple', 'banana', 'grape']\n",
      "Test Fruits (UNSEEN):    ['mango', 'orange']\n",
      "Few-Shot Setting:        5-shot, 15-query\n",
      "Backbone:                resnet18\n",
      "Embedding Dimension:     512\n",
      "Training Episodes:       1000\n",
      "Regularization:          Dropout=0.3, LabelSmooth=0.1\n",
      "Early Stopping:          Patience=10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these based on your dataset and experiments\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration for all experiments\"\"\"\n",
    "    \n",
    "    # Dataset paths - FruitVision Dataset\n",
    "    DATA_ROOT = r\"C:\\Users\\admin\\Desktop\\Amr Samir\\FruitVision\"\n",
    "    \n",
    "    # Fruits for training (SEEN during training)\n",
    "    TRAIN_FRUITS = ['apple', 'banana', 'grape']\n",
    "    \n",
    "    # Fruits for testing (UNSEEN - the key experiment!)\n",
    "    TEST_FRUITS = ['mango', 'orange']\n",
    "    \n",
    "    # Quality classes (binary grading) - FruitVision uses fresh/rotten\n",
    "    CLASSES = ['fresh', 'rotten']  # Maps to Good/Bad\n",
    "    N_CLASSES = 2  # Binary: Fresh (Good) vs Rotten (Bad)\n",
    "    \n",
    "    # Few-shot settings\n",
    "    N_SHOT = 5       # Number of support examples per class (5-shot learning)\n",
    "    N_QUERY = 15     # Number of query examples per class\n",
    "    N_EPISODES_TRAIN = 500    # Reduced for faster epochs, more epochs\n",
    "    N_EPISODES_VAL = 200      # Validation episodes\n",
    "    N_EPISODES_TEST = 600     # Test episodes for statistical significance\n",
    "    \n",
    "    # Model settings\n",
    "    BACKBONE = 'resnet18'     # Options: 'resnet18', 'resnet50', 'efficientnet_b0'\n",
    "    EMBEDDING_DIM = 256       # Reduced for better generalization\n",
    "    PRETRAINED = True         # Use ImageNet pretrained weights\n",
    "    \n",
    "    # Training settings - TUNED TO PREVENT OVERFITTING\n",
    "    EPOCHS = 30               # More epochs with early stopping\n",
    "    LEARNING_RATE = 5e-5      # Lower LR for pretrained backbone\n",
    "    WEIGHT_DECAY = 5e-4       # Stronger L2 regularization\n",
    "    BATCH_SIZE = 1            # For episodic training, batch_size = 1 episode\n",
    "    \n",
    "    # Regularization settings\n",
    "    DROPOUT_RATE = 0.4        # Higher dropout\n",
    "    LABEL_SMOOTHING = 0.1     # Label smoothing for cross-entropy\n",
    "    GRADIENT_CLIP = 1.0       # Gradient clipping norm\n",
    "    EARLY_STOPPING_PATIENCE = 7  # Early stopping patience\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    WARMUP_EPOCHS = 3         # Warmup epochs before full LR\n",
    "    \n",
    "    # Validation split - hold out some images for proper validation\n",
    "    VAL_SPLIT_RATIO = 0.15    # 15% of training images for validation\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 224\n",
    "    \n",
    "    # Paths for saving\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    RESULTS_DIR = './results'\n",
    "    \n",
    "    # Experiment name (for logging)\n",
    "    EXPERIMENT_NAME = f\"ProtoNet_{BACKBONE}_{N_SHOT}shot\"\n",
    "    \n",
    "    # Ablation settings\n",
    "    ABLATION_SHOTS = [1, 3, 5, 10]  # For N-shot ablation study\n",
    "    ABLATION_EPISODES = 300         # Episodes per ablation experiment\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTAL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train Fruits (SEEN):     {config.TRAIN_FRUITS}\")\n",
    "print(f\"Test Fruits (UNSEEN):    {config.TEST_FRUITS}\")\n",
    "print(f\"Few-Shot Setting:        {config.N_SHOT}-shot, {config.N_QUERY}-query\")\n",
    "print(f\"Backbone:                {config.BACKBONE}\")\n",
    "print(f\"Embedding Dimension:     {config.EMBEDDING_DIM}\")\n",
    "print(f\"Training Episodes/Epoch: {config.N_EPISODES_TRAIN}\")\n",
    "print(f\"Learning Rate:           {config.LEARNING_RATE}\")\n",
    "print(f\"Regularization:          Dropout={config.DROPOUT_RATE}, L2={config.WEIGHT_DECAY}\")\n",
    "print(f\"Validation Split:        {config.VAL_SPLIT_RATIO*100:.0f}% held out\")\n",
    "print(f\"Early Stopping:          Patience={config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60821e2a",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation & Transforms\n",
    "\n",
    "Strong augmentation is critical for learning generalizable defect features. We use different transforms for support (stable) and query (augmented) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaff842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data transforms defined (balanced for generalization)\n",
      "  Training: Moderate augmentation\n",
      "  Support: Light augmentation for stable prototypes\n",
      "  Evaluation: Minimal transforms\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA AUGMENTATION STRATEGIES - BALANCED FOR AVOIDING OVERFITTING\n",
    "# ============================================================================\n",
    "\n",
    "# Training augmentation (balanced - not too aggressive)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=20),  # Reduced from 30\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,  # Reduced for stability\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.08))  # Reduced\n",
    "])\n",
    "\n",
    "# Support set transform (light augmentation for stable prototypes)\n",
    "support_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE + 32, config.IMAGE_SIZE + 32)),\n",
    "    transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Validation/Test augmentation (minimal)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"‚úì Data transforms defined (balanced for generalization)\")\n",
    "print(f\"  Training: Moderate augmentation\")\n",
    "print(f\"  Support: Light augmentation for stable prototypes\")\n",
    "print(f\"  Evaluation: Minimal transforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c3c2d",
   "metadata": {},
   "source": [
    "## 3. Episodic Dataset for Few-Shot Learning\n",
    "\n",
    "The core innovation: Instead of traditional batches, we sample **episodes**. Each episode contains:\n",
    "- **Support Set**: K examples of Good + K examples of Bad (used to build prototypes)\n",
    "- **Query Set**: Q examples to classify using the prototypes\n",
    "\n",
    "This forces the model to learn \"defectness\" in a generalizable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e17ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Episodic dataset classes defined (with separate support/query transforms)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EPISODIC DATASET WITH PROPER TRAIN/VAL SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "class FruitQualityDataset:\n",
    "    \"\"\"\n",
    "    Loads fruit images organized by fruit type and quality.\n",
    "    Supports proper train/val split to prevent data leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, fruit_types, transform=None, \n",
    "                 support_transform=None, query_transform=None,\n",
    "                 split='all', val_ratio=0.0, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train', 'val', or 'all' (no split)\n",
    "            val_ratio: fraction of images to hold out for validation\n",
    "            seed: random seed for reproducible splits\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.fruit_types = fruit_types\n",
    "        self.transform = transform\n",
    "        self.support_transform = support_transform or transform\n",
    "        self.query_transform = query_transform or transform\n",
    "        self.classes = ['fresh', 'rotten']\n",
    "        self.split = split\n",
    "        self.val_ratio = val_ratio\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.data = defaultdict(lambda: defaultdict(list))\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all image paths organized by fruit type and quality\"\"\"\n",
    "        rng = np.random.RandomState(self.seed)  # Reproducible split\n",
    "        \n",
    "        for fruit in self.fruit_types:\n",
    "            for quality in self.classes:\n",
    "                folder_path = os.path.join(self.data_root, fruit, quality)\n",
    "                if os.path.exists(folder_path):\n",
    "                    all_images = sorted([\n",
    "                        os.path.join(folder_path, f) \n",
    "                        for f in os.listdir(folder_path) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "                    ])\n",
    "                    \n",
    "                    # Apply train/val split\n",
    "                    if self.val_ratio > 0 and self.split in ['train', 'val']:\n",
    "                        n_val = max(1, int(len(all_images) * self.val_ratio))\n",
    "                        indices = rng.permutation(len(all_images))\n",
    "                        \n",
    "                        if self.split == 'val':\n",
    "                            selected_idx = indices[:n_val]\n",
    "                        else:  # train\n",
    "                            selected_idx = indices[n_val:]\n",
    "                        \n",
    "                        images = [all_images[i] for i in selected_idx]\n",
    "                    else:\n",
    "                        images = all_images\n",
    "                    \n",
    "                    self.data[fruit][quality] = images\n",
    "                    print(f\"  [{self.split:5}] Loaded {len(images):4d} images: {fruit}/{quality}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö† Missing folder: {folder_path}\")\n",
    "    \n",
    "    def get_episode(self, n_shot, n_query, fruit=None):\n",
    "        \"\"\"Sample a single episode with separate transforms\"\"\"\n",
    "        if fruit is None:\n",
    "            fruit = random.choice(self.fruit_types)\n",
    "        \n",
    "        support_images, support_labels = [], []\n",
    "        query_images, query_labels = [], []\n",
    "        \n",
    "        for class_idx, quality in enumerate(self.classes):\n",
    "            all_images = self.data[fruit][quality]\n",
    "            \n",
    "            required = n_shot + n_query\n",
    "            if len(all_images) < required:\n",
    "                # Handle small datasets by sampling with replacement for query\n",
    "                if len(all_images) < n_shot:\n",
    "                    raise ValueError(\n",
    "                        f\"Not enough images for support: {fruit}/{quality}. \"\n",
    "                        f\"Need {n_shot}, have {len(all_images)}\"\n",
    "                    )\n",
    "                support_paths = random.sample(all_images, n_shot)\n",
    "                remaining = [p for p in all_images if p not in support_paths]\n",
    "                if len(remaining) < n_query:\n",
    "                    query_paths = remaining + random.choices(all_images, k=n_query-len(remaining))\n",
    "                else:\n",
    "                    query_paths = random.sample(remaining, n_query)\n",
    "            else:\n",
    "                sampled = random.sample(all_images, required)\n",
    "                support_paths = sampled[:n_shot]\n",
    "                query_paths = sampled[n_shot:]\n",
    "            \n",
    "            # Use support transform for support set\n",
    "            for path in support_paths:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                if self.support_transform:\n",
    "                    img = self.support_transform(img)\n",
    "                support_images.append(img)\n",
    "                support_labels.append(class_idx)\n",
    "            \n",
    "            # Use query transform for query set\n",
    "            for path in query_paths:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                if self.query_transform:\n",
    "                    img = self.query_transform(img)\n",
    "                query_images.append(img)\n",
    "                query_labels.append(class_idx)\n",
    "        \n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, support_labels, query_images, query_labels, fruit\n",
    "\n",
    "\n",
    "class EpisodicDataLoader:\n",
    "    \"\"\"DataLoader that yields episodes instead of batches.\"\"\"\n",
    "    def __init__(self, dataset, n_shot, n_query, n_episodes, fruits=None):\n",
    "        self.dataset = dataset\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        self.fruits = fruits if fruits else dataset.fruit_types\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_episodes):\n",
    "            fruit = random.choice(self.fruits)\n",
    "            yield self.dataset.get_episode(self.n_shot, self.n_query, fruit=fruit)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "print(\"‚úì Episodic dataset with proper train/val split defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560261dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model initialized with regularization\n",
      "  Backbone: resnet18, Dropout: 0.3\n",
      "  Total parameters: 11,703,873\n",
      "  Trainable: 11,703,873\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROTOTYPICAL NETWORK WITH IMPROVED ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"Feature extraction with dropout regularization and frozen early layers.\"\"\"\n",
    "    def __init__(self, backbone='resnet18', embedding_dim=256, \n",
    "                 pretrained=True, dropout_rate=0.4, freeze_bn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        if backbone == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            in_features = self.encoder.fc.in_features\n",
    "            self.encoder.fc = nn.Identity()\n",
    "        elif backbone == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            in_features = self.encoder.fc.in_features\n",
    "            self.encoder.fc = nn.Identity()\n",
    "        elif backbone == 'efficientnet_b0':\n",
    "            self.encoder = models.efficientnet_b0(pretrained=pretrained)\n",
    "            in_features = self.encoder.classifier[1].in_features\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        # Freeze early layers to prevent overfitting\n",
    "        if pretrained:\n",
    "            self._freeze_early_layers()\n",
    "        \n",
    "        # Simpler projection head (less prone to overfitting)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_dim * 2),\n",
    "            nn.BatchNorm1d(embedding_dim * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "        )\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def _freeze_early_layers(self):\n",
    "        \"\"\"Freeze early convolutional layers\"\"\"\n",
    "        # Freeze first conv and first residual block\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if 'layer1' in name or 'conv1' in name or 'bn1' in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        embeddings = self.projection(features)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    \"\"\"Prototypical Network with learnable temperature.\"\"\"\n",
    "    def __init__(self, backbone='resnet18', embedding_dim=256, \n",
    "                 pretrained=True, dropout_rate=0.4, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = EmbeddingNetwork(backbone, embedding_dim, pretrained, dropout_rate)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Initialize temperature lower for normalized embeddings\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "        \n",
    "    def compute_prototypes(self, support_embeddings, support_labels, n_classes=2):\n",
    "        prototypes = torch.zeros(n_classes, self.embedding_dim, \n",
    "                                 device=support_embeddings.device)\n",
    "        for c in range(n_classes):\n",
    "            mask = (support_labels == c)\n",
    "            class_embeddings = support_embeddings[mask]\n",
    "            prototypes[c] = class_embeddings.mean(dim=0)\n",
    "        return prototypes\n",
    "    \n",
    "    def forward(self, support_images, support_labels, query_images, n_classes=2):\n",
    "        # Encode all images in one forward pass (more efficient)\n",
    "        all_images = torch.cat([support_images, query_images], dim=0)\n",
    "        all_embeddings = self.encoder(all_images)\n",
    "        \n",
    "        # Split embeddings\n",
    "        n_support = support_images.size(0)\n",
    "        support_embeddings = all_embeddings[:n_support]\n",
    "        query_embeddings = all_embeddings[n_support:]\n",
    "        \n",
    "        # Compute prototypes and distances\n",
    "        prototypes = self.compute_prototypes(support_embeddings, support_labels, n_classes)\n",
    "        distances = torch.cdist(query_embeddings, prototypes, p=2)\n",
    "        logits = -distances / self.temperature.clamp(min=0.1, max=2.0)\n",
    "        \n",
    "        return logits, query_embeddings, support_embeddings, prototypes\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = PrototypicalNetwork(\n",
    "    backbone=config.BACKBONE,\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    pretrained=config.PRETRAINED,\n",
    "    dropout_rate=config.DROPOUT_RATE,\n",
    "    temperature=0.5  # Lower initial temperature\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model initialized with frozen early layers\")\n",
    "print(f\"  Backbone: {config.BACKBONE}, Dropout: {config.DROPOUT_RATE}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS FUNCTION: Cross-Entropy with Label Smoothing + Optional Contrastive\n",
    "# ============================================================================\n",
    "\n",
    "class PrototypicalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for Prototypical Networks.\n",
    "    - Cross-entropy with label smoothing for classification\n",
    "    - Optional supervised contrastive regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, label_smoothing=0.1, contrastive_weight=0.1, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        self.contrastive_weight = contrastive_weight\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def supervised_contrastive_loss(self, embeddings, labels):\n",
    "        \"\"\"Supervised contrastive loss to improve embedding quality.\"\"\"\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "        \n",
    "        # Create mask for positive pairs (same class, different sample)\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(embeddings.device)\n",
    "        \n",
    "        # Remove diagonal (self-similarity)\n",
    "        mask = mask - torch.eye(batch_size, device=embeddings.device)\n",
    "        \n",
    "        # Compute log softmax\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        # Mask out self-similarity for denominator\n",
    "        exp_sim = exp_sim * (1 - torch.eye(batch_size, device=embeddings.device))\n",
    "        \n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Compute mean of positive pairs\n",
    "        mask_sum = mask.sum(dim=1)\n",
    "        mask_sum = torch.clamp(mask_sum, min=1)  # Avoid division by zero\n",
    "        \n",
    "        loss = -(mask * log_prob).sum(dim=1) / mask_sum\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, logits, labels, embeddings=None, all_labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Classification logits from prototypical network\n",
    "            labels: Ground truth labels for query set\n",
    "            embeddings: Optional - all embeddings for contrastive loss\n",
    "            all_labels: Optional - all labels for contrastive loss\n",
    "        \"\"\"\n",
    "        ce_loss = self.ce_loss(logits, labels)\n",
    "        \n",
    "        total_loss = ce_loss\n",
    "        contrastive_loss = torch.tensor(0.0)\n",
    "        \n",
    "        if embeddings is not None and all_labels is not None and self.contrastive_weight > 0:\n",
    "            contrastive_loss = self.supervised_contrastive_loss(embeddings, all_labels)\n",
    "            total_loss = ce_loss + self.contrastive_weight * contrastive_loss\n",
    "        \n",
    "        return total_loss, ce_loss, contrastive_loss\n",
    "\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = PrototypicalLoss(\n",
    "    label_smoothing=config.LABEL_SMOOTHING,\n",
    "    contrastive_weight=0.1,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(f\"‚úì Loss function initialized\")\n",
    "print(f\"  Label smoothing: {config.LABEL_SMOOTHING}\")\n",
    "print(f\"  Contrastive weight: 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training utilities with early stopping & gradient clipping\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING UTILITIES WITH IMPROVED MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        improved = (score > self.best_score + self.min_delta) if self.mode == 'max' \\\n",
    "                   else (score < self.best_score - self.min_delta)\n",
    "            \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    return (predictions == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_clip=1.0):\n",
    "    \"\"\"Train one epoch with gradient clipping.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, n_episodes = 0.0, 0.0, 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for support_imgs, support_lbls, query_imgs, query_lbls, fruit in pbar:\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_lbls = support_lbls.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        query_lbls = query_lbls.to(device)\n",
    "        \n",
    "        # Forward pass - now returns support embeddings too (no double computation)\n",
    "        logits, query_emb, support_emb, _ = model(support_imgs, support_lbls, query_imgs)\n",
    "        \n",
    "        all_embeddings = torch.cat([support_emb, query_emb], dim=0)\n",
    "        all_labels = torch.cat([support_lbls, query_lbls], dim=0)\n",
    "        \n",
    "        loss, _, _ = criterion(logits, query_lbls, all_embeddings, all_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if gradient_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = compute_accuracy(logits, query_lbls)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        n_episodes += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{acc:.3f}'})\n",
    "    \n",
    "    return total_loss / n_episodes, total_acc / n_episodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, desc=\"Evaluating\", return_all=False):\n",
    "    \"\"\"Evaluate with option to return all episode accuracies.\"\"\"\n",
    "    model.eval()\n",
    "    all_accuracies = []\n",
    "    fruit_accuracies = defaultdict(list)\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    for support_imgs, support_lbls, query_imgs, query_lbls, fruit in pbar:\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_lbls = support_lbls.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        query_lbls = query_lbls.to(device)\n",
    "        \n",
    "        logits, _, _, _ = model(support_imgs, support_lbls, query_imgs)\n",
    "        acc = compute_accuracy(logits, query_lbls)\n",
    "        all_accuracies.append(acc)\n",
    "        fruit_accuracies[fruit].append(acc)\n",
    "        pbar.set_postfix({'acc': f'{acc:.3f}'})\n",
    "    \n",
    "    mean_acc = np.mean(all_accuracies)\n",
    "    std_acc = np.std(all_accuracies)\n",
    "    per_fruit_acc = {fruit: np.mean(accs) for fruit, accs in fruit_accuracies.items()}\n",
    "    \n",
    "    if return_all:\n",
    "        return mean_acc, per_fruit_acc, all_accuracies, dict(fruit_accuracies)\n",
    "    return mean_acc, per_fruit_acc\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(), 'metrics': metrics\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['metrics']\n",
    "\n",
    "print(\"‚úì Training utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATASETS\n",
      "============================================================\n",
      "Loading TRAINING data (seen fruits):\n",
      "  Loaded  765 images: apple/fresh\n",
      "  Loaded  630 images: apple/rotten\n",
      "  Loaded  749 images: banana/fresh\n",
      "  Loaded  632 images: banana/rotten\n",
      "  Loaded  770 images: grape/fresh\n",
      "  Loaded  630 images: grape/rotten\n",
      "\n",
      "Loading TEST data (UNSEEN fruits):\n",
      "  Loaded  763 images: mango/fresh\n",
      "  Loaded  630 images: mango/rotten\n",
      "  Loaded  753 images: orange/fresh\n",
      "  Loaded  656 images: orange/rotten\n",
      "\n",
      "‚úì DataLoaders: Train=1000, Val=200, Test=600\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATASET WITH PROPER TRAIN/VAL SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATASETS WITH TRAIN/VAL SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not os.path.exists(config.DATA_ROOT):\n",
    "    print(f\"‚ö†Ô∏è  Dataset not found at: {config.DATA_ROOT}\")\n",
    "else:\n",
    "    # Training set (80-85% of seen fruit images)\n",
    "    print(\"\\nLoading TRAINING data (seen fruits - train split):\")\n",
    "    train_dataset = FruitQualityDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        fruit_types=config.TRAIN_FRUITS,\n",
    "        support_transform=support_transform,\n",
    "        query_transform=train_transform,\n",
    "        split='train',\n",
    "        val_ratio=config.VAL_SPLIT_RATIO,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Validation set (15-20% of seen fruit images - HELD OUT)\n",
    "    print(\"\\nLoading VALIDATION data (seen fruits - val split, HELD OUT):\")\n",
    "    val_dataset = FruitQualityDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        fruit_types=config.TRAIN_FRUITS,\n",
    "        support_transform=eval_transform,\n",
    "        query_transform=eval_transform,\n",
    "        split='val',\n",
    "        val_ratio=config.VAL_SPLIT_RATIO,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Test set (unseen fruits - no split needed)\n",
    "    print(\"\\nLoading TEST data (UNSEEN fruits):\")\n",
    "    test_dataset = FruitQualityDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        fruit_types=config.TEST_FRUITS,\n",
    "        support_transform=eval_transform,\n",
    "        query_transform=eval_transform,\n",
    "        split='all'\n",
    "    )\n",
    "    \n",
    "    train_loader = EpisodicDataLoader(train_dataset, config.N_SHOT, \n",
    "                                       config.N_QUERY, config.N_EPISODES_TRAIN)\n",
    "    val_loader = EpisodicDataLoader(val_dataset, config.N_SHOT, \n",
    "                                     config.N_QUERY, config.N_EPISODES_VAL)\n",
    "    test_loader = EpisodicDataLoader(test_dataset, config.N_SHOT, \n",
    "                                      config.N_QUERY, config.N_EPISODES_TEST)\n",
    "    \n",
    "    print(f\"\\n‚úì DataLoaders created:\")\n",
    "    print(f\"  Train: {len(train_loader)} episodes (from train split)\")\n",
    "    print(f\"  Val:   {len(val_loader)} episodes (from held-out val split)\")\n",
    "    print(f\"  Test:  {len(test_loader)} episodes (unseen fruits)\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Validation uses DIFFERENT images than training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c2a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING WITH REGULARIZATION\n",
      "Early stopping patience: 10\n",
      "Gradient clipping: 1.0\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=2.2775, Acc=0.941\n",
      "  Val: Acc=0.895 | Gap=0.046 ‚úì\n",
      "  ‚úì Best model saved\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=2.0345, Acc=0.977\n",
      "  Val: Acc=0.904 | Gap=0.073 ‚úì\n",
      "  ‚úì Best model saved\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.9006, Acc=0.987\n",
      "  Val: Acc=0.850 | Gap=0.137 ‚ö†Ô∏è OVERFITTING\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.8443, Acc=0.991\n",
      "  Val: Acc=0.889 | Gap=0.102 ‚ö†Ô∏è OVERFITTING\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.8123, Acc=0.993\n",
      "  Val: Acc=0.915 | Gap=0.078 ‚úì\n",
      "  ‚úì Best model saved\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 927/1000 [20:25<01:40,  1.38s/it, loss=1.6966, acc=1.000]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.7000, Acc=0.999\n",
      "  Val: Acc=0.913 | Gap=0.086 ‚úì\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 988/1000 [22:56<00:16,  1.34s/it, loss=1.6801, acc=1.000]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.6924, Acc=0.999\n",
      "  Val: Acc=0.938 | Gap=0.061 ‚úì\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.6956, Acc=0.999\n",
      "  Val: Acc=0.958 | Gap=0.042 ‚úì\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.6963, Acc=0.999\n",
      "  Val: Acc=0.906 | Gap=0.093 ‚úì\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: Loss=1.7258, Acc=0.997\n",
      "  Val: Acc=0.921 | Gap=0.076 ‚úì\n",
      "\n",
      "‚ö†Ô∏è Early stopping at epoch 31\n",
      "\n",
      "‚úì Training complete. Best val acc: 0.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING WITH WARMUP, EARLY STOPPING, AND OVERFITTING DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_protonet(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Training loop with warmup, early stopping, and detailed monitoring.\"\"\"\n",
    "    \n",
    "    # Separate parameter groups with different learning rates\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.encoder.encoder.parameters(), 'lr': config.LEARNING_RATE * 0.1, 'name': 'backbone'},\n",
    "        {'params': model.encoder.projection.parameters(), 'lr': config.LEARNING_RATE, 'name': 'projection'},\n",
    "        {'params': [model.temperature], 'lr': config.LEARNING_RATE * 0.5, 'name': 'temperature'}\n",
    "    ], weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    # Warmup + Cosine annealing\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < config.WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / config.WARMUP_EPOCHS  # Linear warmup\n",
    "        else:\n",
    "            progress = (epoch - config.WARMUP_EPOCHS) / (config.EPOCHS - config.WARMUP_EPOCHS)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))  # Cosine decay\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=config.EARLY_STOPPING_PATIENCE, min_delta=0.003)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_acc': [], 'lr': [], 'temperature': [], 'gap': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING WITH WARMUP AND EARLY STOPPING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Warmup epochs: {config.WARMUP_EPOCHS}\")\n",
    "    print(f\"Early stopping patience: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"Gradient clipping: {config.GRADIENT_CLIP}\")\n",
    "    print(f\"Initial LR: backbone={config.LEARNING_RATE*0.1:.1e}, projection={config.LEARNING_RATE:.1e}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, config.GRADIENT_CLIP\n",
    "        )\n",
    "        \n",
    "        # Validation on HELD-OUT data\n",
    "        val_acc, val_per_fruit = evaluate(model, val_loader, device, \"Validating\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        gap = train_acc - val_acc\n",
    "        temp = model.temperature.item()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        history['temperature'].append(temp)\n",
    "        history['gap'].append(gap)\n",
    "        \n",
    "        # Determine training status\n",
    "        if epoch <= config.WARMUP_EPOCHS:\n",
    "            status = \"üî• WARMUP\"\n",
    "        elif gap > 0.15:\n",
    "            status = \"üî¥ OVERFITTING\"\n",
    "        elif gap > 0.08:\n",
    "            status = \"üü° SLIGHT OVERFIT\"\n",
    "        elif val_acc < 0.55:\n",
    "            status = \"üü° UNDERFITTING\"\n",
    "        else:\n",
    "            status = \"üü¢ GOOD\"\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch}/{config.EPOCHS} {status}\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
    "        print(f\"  Val:   Acc={val_acc:.3f} | Gap={gap:+.3f} | Temp={temp:.3f}\")\n",
    "        print(f\"  LR:    {current_lr:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint(model, optimizer, epoch, \n",
    "                          {'val_acc': val_acc, 'train_acc': train_acc, 'gap': gap}, \n",
    "                          os.path.join(config.CHECKPOINT_DIR, 'best_model.pth'))\n",
    "            print(f\"  ‚úì Best model saved\")\n",
    "        \n",
    "        # Early stopping (only after warmup)\n",
    "        if epoch > config.WARMUP_EPOCHS:\n",
    "            if early_stopping(val_acc, epoch):\n",
    "                print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch}\")\n",
    "                print(f\"   Best val acc: {early_stopping.best_score:.3f} at epoch {early_stopping.best_epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.3f}\")\n",
    "    print(f\"Final train/val gap: {history['gap'][-1]:+.3f}\")\n",
    "    \n",
    "    if best_val_acc > 0.85:\n",
    "        print(\"‚úÖ Model achieved strong performance\")\n",
    "    elif best_val_acc > 0.70:\n",
    "        print(\"‚úÖ Model achieved reasonable performance\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Model may need more training or tuning\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Train the model\n",
    "if os.path.exists(config.DATA_ROOT):\n",
    "    history = train_protonet(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e000061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CURVES WITH OVERFITTING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training curves with detailed overfitting analysis.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0,0].plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].set_title('Training Loss')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Plot 2: Accuracy with gap visualization\n",
    "    axes[0,1].plot(epochs, history['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0,1].plot(epochs, history['val_acc'], 'orange', label='Validation', linewidth=2)\n",
    "    axes[0,1].fill_between(epochs, history['val_acc'], history['train_acc'],\n",
    "                           alpha=0.3, color='red', label='Overfit Gap')\n",
    "    axes[0,1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].set_title('Train vs Validation Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].set_ylim(0.4, 1.0)\n",
    "    \n",
    "    # Plot 3: Train-Val Gap\n",
    "    axes[1,0].plot(epochs, history['gap'], 'r-', linewidth=2)\n",
    "    axes[1,0].axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='Warning (0.1)')\n",
    "    axes[1,0].axhline(y=0.15, color='red', linestyle='--', alpha=0.7, label='Overfitting (0.15)')\n",
    "    axes[1,0].axhline(y=0, color='green', linestyle='--', alpha=0.7, label='Ideal (0)')\n",
    "    axes[1,0].fill_between(epochs, 0, history['gap'], \n",
    "                           where=[g > 0.1 for g in history['gap']], \n",
    "                           alpha=0.3, color='red')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Train - Val Accuracy')\n",
    "    axes[1,0].set_title('Overfitting Gap')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Learning Rate and Temperature\n",
    "    ax4 = axes[1,1]\n",
    "    ax4.plot(epochs, history['lr'], 'g-', linewidth=2, label='Learning Rate')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Learning Rate', color='green')\n",
    "    ax4.tick_params(axis='y', labelcolor='green')\n",
    "    ax4.set_title('Learning Rate & Temperature Schedule')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    if 'temperature' in history:\n",
    "        ax4_twin = ax4.twinx()\n",
    "        ax4_twin.plot(epochs, history['temperature'], 'm--', linewidth=2, label='Temperature')\n",
    "        ax4_twin.set_ylabel('Temperature', color='magenta')\n",
    "        ax4_twin.tick_params(axis='y', labelcolor='magenta')\n",
    "    \n",
    "    # Add legend\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    if 'temperature' in history:\n",
    "        lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "        ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    else:\n",
    "        ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nüìä TRAINING ANALYSIS:\")\n",
    "    print(f\"  Final train accuracy: {history['train_acc'][-1]:.3f}\")\n",
    "    print(f\"  Final val accuracy:   {history['val_acc'][-1]:.3f}\")\n",
    "    print(f\"  Final gap:            {history['gap'][-1]:+.3f}\")\n",
    "    \n",
    "    max_gap = max(history['gap'])\n",
    "    if max_gap > 0.15:\n",
    "        print(f\"  ‚ö†Ô∏è Maximum gap was {max_gap:.3f} - some overfitting occurred\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Maximum gap was {max_gap:.3f} - overfitting well controlled\")\n",
    "\n",
    "\n",
    "# Plot if training was run\n",
    "if 'history' in dir() and history:\n",
    "    plot_training_history(history, os.path.join(config.RESULTS_DIR, 'training_curves.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f6a8d",
   "metadata": {},
   "source": [
    "## 9. KEY EXPERIMENT: Test on UNSEEN Fruits üéØ\n",
    "\n",
    "**This is the core contribution of your thesis!**\n",
    "\n",
    "The model was trained ONLY on Apple, Banana, Grape. Now we test on Mango and Orange (which the model has NEVER seen). If it performs well, we have proven that it learned \"defectness\" rather than fruit-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ KEY EXPERIMENT: TESTING ON UNSEEN FRUIT SPECIES\n",
      "======================================================================\n",
      "Trained on: ['apple', 'banana', 'grape']\n",
      "Testing on: ['mango', 'orange'] (UNSEEN)\n",
      "Setting: 5-shot, 5 trials\n",
      "‚úì Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTS WITH 95% CONFIDENCE INTERVALS\n",
      "======================================================================\n",
      "\n",
      "Metric                     Mean        Std          95% CI\n",
      "-------------------------------------------------------\n",
      "Overall                   0.921      0.003 ¬±        0.003\n",
      "\n",
      "Per-Fruit:\n",
      "  Mango                 0.918      0.004 ¬±        0.005\n",
      "    Episode range: [0.73, 1.00]\n",
      "  Orange                0.923      0.003 ¬±        0.004\n",
      "    Episode range: [0.70, 1.00]\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EXCELLENT: Strong cross-species generalization (>85%)\n",
      "\n",
      "üìù LaTeX Table Row:\n",
      "ProtoNet+SupCon & 91.8 $\\pm$ 0.5 & 92.3 $\\pm$ 0.4 & 92.1 $\\pm$ 0.3 \\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KEY EXPERIMENT: TEST WITH 95% CONFIDENCE INTERVALS\n",
    "# ============================================================================\n",
    "from scipy import stats\n",
    "\n",
    "def compute_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Compute mean and 95% CI.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "    se = std / np.sqrt(n)\n",
    "    t_val = stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    ci = t_val * se\n",
    "    return mean, std, ci\n",
    "\n",
    "\n",
    "def test_on_unseen_fruits(model, test_dataset, device, config, n_trials=5):\n",
    "    \"\"\"Test on unseen fruits with proper confidence intervals.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ KEY EXPERIMENT: TESTING ON UNSEEN FRUIT SPECIES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Trained on: {config.TRAIN_FRUITS}\")\n",
    "    print(f\"Testing on: {config.TEST_FRUITS} (UNSEEN)\")\n",
    "    print(f\"Setting: {config.N_SHOT}-shot, {n_trials} trials\")\n",
    "    \n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n",
    "        print(\"‚úì Loaded best model\")\n",
    "    \n",
    "    all_trial_accs = []\n",
    "    per_fruit_trials = defaultdict(list)\n",
    "    all_episode_accs = defaultdict(list)\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        test_loader = EpisodicDataLoader(\n",
    "            test_dataset, config.N_SHOT, config.N_QUERY, config.N_EPISODES_TEST\n",
    "        )\n",
    "        acc, per_fruit, all_accs, fruit_accs = evaluate(\n",
    "            model, test_loader, device, f\"Trial {trial+1}/{n_trials}\", return_all=True\n",
    "        )\n",
    "        all_trial_accs.append(acc)\n",
    "        for fruit, f_acc in per_fruit.items():\n",
    "            per_fruit_trials[fruit].append(f_acc)\n",
    "            all_episode_accs[fruit].extend(fruit_accs[fruit])\n",
    "    \n",
    "    # Compute statistics\n",
    "    overall_mean, overall_std, overall_ci = compute_confidence_interval(all_trial_accs)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä RESULTS WITH 95% CONFIDENCE INTERVALS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'95% CI':>15}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Overall':<20} {overall_mean:>10.3f} {overall_std:>10.3f} ¬±{overall_ci:>13.3f}\")\n",
    "    \n",
    "    test_results = {'overall': {'mean': overall_mean, 'std': overall_std, 'ci_95': overall_ci}, \n",
    "                    'per_fruit': {}}\n",
    "    \n",
    "    print(\"\\nPer-Fruit:\")\n",
    "    for fruit in config.TEST_FRUITS:\n",
    "        if fruit in per_fruit_trials:\n",
    "            f_mean, f_std, f_ci = compute_confidence_interval(per_fruit_trials[fruit])\n",
    "            e_accs = all_episode_accs[fruit]\n",
    "            test_results['per_fruit'][fruit] = {\n",
    "                'mean': f_mean, 'std': f_std, 'ci_95': f_ci,\n",
    "                'min': min(e_accs), 'max': max(e_accs)\n",
    "            }\n",
    "            print(f\"  {fruit.capitalize():<16} {f_mean:>10.3f} {f_std:>10.3f} ¬±{f_ci:>13.3f}\")\n",
    "            print(f\"    Episode range: [{min(e_accs):.2f}, {max(e_accs):.2f}]\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    if overall_mean > 0.85:\n",
    "        print(\"‚úÖ EXCELLENT: Strong cross-species generalization (>85%)\")\n",
    "    elif overall_mean > 0.75:\n",
    "        print(\"‚úÖ GOOD: Reasonable generalization (75-85%)\")\n",
    "    elif overall_mean > 0.65:\n",
    "        print(\"‚ö†Ô∏è MODERATE: Room for improvement (65-75%)\")\n",
    "    else:\n",
    "        print(\"‚ùå Needs improvement (<65%)\")\n",
    "    \n",
    "    # LaTeX output\n",
    "    print(\"\\nüìù LaTeX Table Row:\")\n",
    "    latex = f\"ProtoNet+SupCon & \"\n",
    "    for fruit in config.TEST_FRUITS:\n",
    "        if fruit in test_results['per_fruit']:\n",
    "            f = test_results['per_fruit'][fruit]\n",
    "            latex += f\"{f['mean']*100:.1f} $\\\\pm$ {f['ci_95']*100:.1f} & \"\n",
    "    latex += f\"{overall_mean*100:.1f} $\\\\pm$ {overall_ci*100:.1f} \\\\\\\\\"\n",
    "    print(latex)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Run experiment\n",
    "if os.path.exists(config.DATA_ROOT):\n",
    "    test_results = test_on_unseen_fruits(model, test_dataset, device, config, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä N-SHOT ABLATION STUDY\n",
      "======================================================================\n",
      "\n",
      "Testing 1-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-shot: 0.906 ¬± 0.004\n",
      "\n",
      "Testing 3-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3-shot: 0.920 ¬± 0.004\n",
      "\n",
      "Testing 5-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5-shot: 0.920 ¬± 0.007\n",
      "\n",
      "Testing 10-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10-shot: 0.918 ¬± 0.012\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù LaTeX Table:\n",
      "\\begin{tabular}{ccc}\n",
      "K-shot & Accuracy & 95\\% CI \\\\\\hline\n",
      "1 & 90.6\\% & $\\pm$0.4\\% \\\\\n",
      "3 & 92.0\\% & $\\pm$0.4\\% \\\\\n",
      "5 & 92.0\\% & $\\pm$0.7\\% \\\\\n",
      "10 & 91.8\\% & $\\pm$1.2\\% \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# N-SHOT ABLATION STUDY\n",
    "# ============================================================================\n",
    "\n",
    "def ablation_n_shot(model, test_dataset, device, config, \n",
    "                    shots=[1, 3, 5, 10], n_episodes=300, n_trials=3):\n",
    "    \"\"\"Ablation: How does performance change with number of shots?\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä N-SHOT ABLATION STUDY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n",
    "    \n",
    "    ablation_results = {}\n",
    "    \n",
    "    for n_shot in shots:\n",
    "        print(f\"\\nTesting {n_shot}-shot...\")\n",
    "        trial_accs = []\n",
    "        per_fruit_trials = defaultdict(list)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            loader = EpisodicDataLoader(test_dataset, n_shot, 15, n_episodes)\n",
    "            acc, per_fruit = evaluate(model, loader, device, f\"{n_shot}-shot T{trial+1}\")\n",
    "            trial_accs.append(acc)\n",
    "            for fruit, f_acc in per_fruit.items():\n",
    "                per_fruit_trials[fruit].append(f_acc)\n",
    "        \n",
    "        mean, std, ci = compute_confidence_interval(trial_accs)\n",
    "        ablation_results[n_shot] = {\n",
    "            'mean': mean, 'std': std, 'ci_95': ci,\n",
    "            'per_fruit': {f: {'mean': np.mean(a), 'std': np.std(a)} \n",
    "                         for f, a in per_fruit_trials.items()}\n",
    "        }\n",
    "        print(f\"  {n_shot}-shot: {mean:.3f} ¬± {ci:.3f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Overall\n",
    "    means = [ablation_results[s]['mean'] for s in shots]\n",
    "    cis = [ablation_results[s]['ci_95'] for s in shots]\n",
    "    axes[0].errorbar(shots, means, yerr=cis, fmt='bo-', linewidth=2, \n",
    "                     markersize=10, capsize=5)\n",
    "    axes[0].fill_between(shots, [m-c for m,c in zip(means, cis)],\n",
    "                         [m+c for m,c in zip(means, cis)], alpha=0.2)\n",
    "    axes[0].set_xlabel('Number of Shots (K)')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('N-Shot Ablation Study')\n",
    "    axes[0].set_xticks(shots)\n",
    "    axes[0].set_ylim(0.5, 1.0)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Per-fruit\n",
    "    for fruit in config.TEST_FRUITS:\n",
    "        fruit_means = [ablation_results[s]['per_fruit'].get(fruit, {}).get('mean', 0) for s in shots]\n",
    "        axes[1].plot(shots, fruit_means, 'o-', linewidth=2, label=fruit.capitalize())\n",
    "    axes[1].set_xlabel('Number of Shots (K)')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Per-Fruit Performance')\n",
    "    axes[1].set_xticks(shots)\n",
    "    axes[1].set_ylim(0.5, 1.0)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'ablation_nshot.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # LaTeX table\n",
    "    print(\"\\nüìù LaTeX Table:\")\n",
    "    print(\"\\\\begin{tabular}{ccc}\")\n",
    "    print(\"K-shot & Accuracy & 95\\\\% CI \\\\\\\\\\\\hline\")\n",
    "    for s in shots:\n",
    "        r = ablation_results[s]\n",
    "        print(f\"{s} & {r['mean']*100:.1f}\\\\% & $\\\\pm${r['ci_95']*100:.1f}\\\\% \\\\\\\\\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    \n",
    "    return ablation_results\n",
    "\n",
    "# Run ablation\n",
    "if os.path.exists(config.DATA_ROOT):\n",
    "    nshot_results = ablation_n_shot(model, test_dataset, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE COMPARISONS WITH CONFIDENCE INTERVALS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_supervised_baseline(backbone_name, train_dataset, test_dataset, \n",
    "                                  device, epochs=15, n_trials=3):\n",
    "    \"\"\"\n",
    "    Train supervised baseline and test on unseen fruits.\n",
    "    Returns accuracy with confidence intervals for fair comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE: Supervised {backbone_name.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Simple dataset class\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, fruit_dataset, transform):\n",
    "            self.images, self.labels = [], []\n",
    "            for fruit in fruit_dataset.fruit_types:\n",
    "                for qi, quality in enumerate(['fresh', 'rotten']):\n",
    "                    for path in fruit_dataset.data[fruit][quality]:\n",
    "                        self.images.append(path)\n",
    "                        self.labels.append(qi)\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self): return len(self.images)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img = Image.open(self.images[idx]).convert('RGB')\n",
    "            return self.transform(img), self.labels[idx]\n",
    "    \n",
    "    trial_accs = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        set_seed(42 + trial)  # Different seed per trial\n",
    "        \n",
    "        # Create model\n",
    "        if backbone_name == 'densenet121':\n",
    "            baseline_model = models.densenet121(pretrained=True)\n",
    "            baseline_model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3), \n",
    "                nn.Linear(baseline_model.classifier.in_features, 2)\n",
    "            )\n",
    "        elif backbone_name == 'vgg16':\n",
    "            baseline_model = models.vgg16(pretrained=True)\n",
    "            baseline_model.classifier[-1] = nn.Linear(4096, 2)\n",
    "        else:  # resnet18\n",
    "            baseline_model = models.resnet18(pretrained=True)\n",
    "            baseline_model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.3), \n",
    "                nn.Linear(baseline_model.fc.in_features, 2)\n",
    "            )\n",
    "        \n",
    "        baseline_model = baseline_model.to(device)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_ds = SimpleDataset(train_dataset, train_transform)\n",
    "        test_ds = SimpleDataset(test_dataset, eval_transform)\n",
    "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "        test_dl = DataLoader(test_ds, batch_size=32, num_workers=0)\n",
    "        \n",
    "        # Train\n",
    "        optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        ce_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        baseline_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for imgs, lbls in tqdm(train_dl, desc=f\"Trial {trial+1} Epoch {epoch+1}\", leave=False):\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = ce_criterion(baseline_model(imgs), lbls)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Test on UNSEEN fruits\n",
    "        baseline_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in test_dl:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                preds = baseline_model(imgs).argmax(1)\n",
    "                correct += (preds == lbls).sum().item()\n",
    "                total += lbls.size(0)\n",
    "        \n",
    "        acc = correct / total\n",
    "        trial_accs.append(acc)\n",
    "        print(f\"  Trial {trial+1}: {acc:.3f}\")\n",
    "    \n",
    "    mean, std, ci = compute_confidence_interval(trial_accs)\n",
    "    print(f\"  Result: {mean:.3f} ¬± {ci:.3f} (95% CI)\")\n",
    "    \n",
    "    return {'mean': mean, 'std': std, 'ci_95': ci, 'trials': trial_accs}\n",
    "\n",
    "\n",
    "def evaluate_clip_baseline(test_dataset, device, n_trials=3):\n",
    "    \"\"\"\n",
    "    Evaluate zero-shot CLIP on unseen fruits.\n",
    "    CLIP uses text prompts without any training on fruit data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BASELINE: Zero-Shot CLIP\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        import clip\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è CLIP not installed. Install with: pip install git+https://github.com/openai/CLIP.git\")\n",
    "        print(\"  Returning placeholder results...\")\n",
    "        return {'mean': 0.5, 'std': 0.0, 'ci_95': 0.0, 'trials': [0.5], 'note': 'CLIP not installed'}\n",
    "    \n",
    "    # Load CLIP model\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model.eval()\n",
    "    \n",
    "    # Text prompts for zero-shot classification\n",
    "    text_prompts = [\n",
    "        \"a photo of a fresh fruit\",\n",
    "        \"a photo of a rotten fruit\"\n",
    "    ]\n",
    "    # Alternative prompts to try\n",
    "    alt_prompts = [\n",
    "        [\"fresh healthy fruit\", \"rotten spoiled fruit\"],\n",
    "        [\"good quality fruit\", \"bad quality fruit\"],\n",
    "        [\"a fresh fruit with no defects\", \"a rotten fruit with visible decay\"]\n",
    "    ]\n",
    "    \n",
    "    trial_accs = []\n",
    "    \n",
    "    for trial, prompts in enumerate([text_prompts] + alt_prompts[:n_trials-1]):\n",
    "        if trial >= n_trials:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\n  Trial {trial+1} prompts: {prompts}\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        text_tokens = clip.tokenize(prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for fruit in test_dataset.fruit_types:\n",
    "            for class_idx, quality in enumerate(['fresh', 'rotten']):\n",
    "                images = test_dataset.data[fruit][quality]\n",
    "                \n",
    "                for img_path in tqdm(images, desc=f\"{fruit}/{quality}\", leave=False):\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img_input = clip_preprocess(img).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        image_features = clip_model.encode_image(img_input)\n",
    "                        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                        \n",
    "                        # Compute similarity\n",
    "                        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "                        pred = similarity.argmax().item()\n",
    "                    \n",
    "                    correct += (pred == class_idx)\n",
    "                    total += 1\n",
    "        \n",
    "        acc = correct / total\n",
    "        trial_accs.append(acc)\n",
    "        print(f\"    Accuracy: {acc:.3f}\")\n",
    "    \n",
    "    mean, std, ci = compute_confidence_interval(trial_accs)\n",
    "    print(f\"\\n  CLIP Result: {mean:.3f} ¬± {ci:.3f} (95% CI)\")\n",
    "    \n",
    "    return {'mean': mean, 'std': std, 'ci_95': ci, 'trials': trial_accs}\n",
    "\n",
    "\n",
    "def run_all_baselines(train_dataset, test_dataset, device, our_result=None):\n",
    "    \"\"\"\n",
    "    Run all baseline comparisons with proper confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Dataset with seen fruits for training baselines\n",
    "        test_dataset: Dataset with unseen fruits for evaluation\n",
    "        device: torch device\n",
    "        our_result: Dictionary with our method's results (from test_on_unseen_fruits)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä COMPREHENSIVE BASELINE COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"All methods evaluated on UNSEEN fruits: {config.TEST_FRUITS}\")\n",
    "    print(f\"Using multiple trials with 95% confidence intervals\")\n",
    "    \n",
    "    baseline_comparison_results = {}\n",
    "    \n",
    "    # Supervised baselines\n",
    "    for backbone in ['resnet18', 'densenet121', 'vgg16']:\n",
    "        baseline_comparison_results[f'Supervised {backbone.upper()}'] = evaluate_supervised_baseline(\n",
    "            backbone, train_dataset, test_dataset, device, epochs=15, n_trials=3\n",
    "        )\n",
    "    \n",
    "    # CLIP baseline (zero-shot)\n",
    "    baseline_comparison_results['Zero-Shot CLIP'] = evaluate_clip_baseline(test_dataset, device, n_trials=3)\n",
    "    \n",
    "    # Add our result if provided\n",
    "    if our_result is not None:\n",
    "        baseline_comparison_results['Ours (ProtoNet 5-shot)'] = {\n",
    "            'mean': our_result['overall']['mean'],\n",
    "            'std': our_result['overall']['std'],\n",
    "            'ci_95': our_result['overall']['ci_95']\n",
    "        }\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä RESULTS COMPARISON TABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n{'Method':<30} {'Accuracy':>12} {'95% CI':>12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for method, res in baseline_comparison_results.items():\n",
    "        print(f\"{method:<30} {res['mean']:>11.1%} ¬±{res['ci_95']:>10.1%}\")\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    methods = list(baseline_comparison_results.keys())\n",
    "    means = [baseline_comparison_results[m]['mean'] for m in methods]\n",
    "    cis = [baseline_comparison_results[m]['ci_95'] for m in methods]\n",
    "    \n",
    "    # Color scheme: blue for baselines, green for ours\n",
    "    colors = ['#3498db' if 'Ours' not in m else '#2ecc71' for m in methods]\n",
    "    \n",
    "    bars = ax.bar(range(len(methods)), means, yerr=cis, capsize=5, \n",
    "                  color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xticks(range(len(methods)))\n",
    "    ax.set_xticklabels(methods, rotation=15, ha='right', fontsize=10)\n",
    "    ax.set_ylabel('Accuracy on Unseen Fruits', fontsize=12)\n",
    "    ax.set_title('Baseline Comparison: Generalization to Unseen Fruit Species\\n(Error bars = 95% CI)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Random Chance')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean, ci in zip(bars, means, cis):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, mean + ci + 0.02, \n",
    "                f'{mean:.1%}', ha='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'baseline_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # LaTeX table\n",
    "    print(\"\\nüìù LaTeX Table:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{Comparison on Unseen Fruit Species}\")\n",
    "    print(\"\\\\begin{tabular}{lcc}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\"Method & Accuracy & 95\\\\% CI \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    for method, res in baseline_comparison_results.items():\n",
    "        clean_method = method.replace('_', '\\\\_')\n",
    "        print(f\"{clean_method} & {res['mean']*100:.1f}\\\\% & $\\\\pm${res['ci_95']*100:.1f}\\\\% \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    \n",
    "    return baseline_comparison_results\n",
    "\n",
    "\n",
    "# Run baselines (pass test_results from the previous experiment)\n",
    "if os.path.exists(config.DATA_ROOT):\n",
    "    # Get our result from the previous experiment\n",
    "    our_test_result = test_results if 'test_results' in dir() else None\n",
    "    baseline_results = run_all_baselines(train_dataset, test_dataset, device, our_test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dac78",
   "metadata": {},
   "source": [
    "## 13. Training Curves Visualization\n",
    "\n",
    "If you need to re-plot the training history after running the training cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: c:\\Users\\Amr Samir\\Documents\\GitHub\\Fruit_type_classification\\FewShot.ipynb\n",
    "# ============================================================================\n",
    "# RE-PLOT TRAINING CURVES (if needed)\n",
    "# ============================================================================\n",
    "\n",
    "# Plot if training was run\n",
    "if 'history' in dir() and history:\n",
    "    plot_training_history(history, os.path.join(config.RESULTS_DIR, 'training_curves.png'))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history found. Run training first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
